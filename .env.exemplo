# Configurações de Ambiente (Local vs Produção/Nuvem)
# Define se os modelos locais (ex: Ollama) devem ser usados. Valores: true ou false
IS_LOCAL=true

# Configurações para uso Local (se IS_LOCAL=true)
# Nome do modelo a rodar no ambiente local
MODEL_LOCAL=llama3.2
# URL base do provedor local (ex: do Ollama)
OLLAMA_BASE_URL=http://localhost:11434/v1
# Chave de API, caso seu provedor local precise (opcional no Ollama padrão, mas pode requerer algo como "ollama")
API_KEY_LOCAL=ollama

# Configurações para Nuvem / Produção (se IS_LOCAL=false)
# Nome do modelo a ser usado (ex: gpt-4o, claude-3-opus, gemini-1.5-pro, etc.)
MODEL=gpt-4o

# Chaves de API para os diferentes provedores de IA (preencha as que for utilizar)
OPENAI_API_KEY=sua_chave_openai_aqui
ANTHROPIC_API_KEY=sua_chave_anthropic_aqui
GEMINI_API_KEY=sua_chave_gemini_aqui

# Token de autenticação do Logfire (obrigatório para instrumentação se não usar via `logfire auth`)
LOGFIRE_API_KEY=seu_token_logfire_aqui
