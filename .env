# Configurações de Ambiente (Local vs Produção/Nuvem)
# Define se os modelos locais (ex: Ollama) devem ser usados. Valores: true ou false
IS_LOCAL=false

# Configurações para uso Local (se IS_LOCAL=true)
# Nome do modelo a rodar no ambiente local
MODEL_LOCAL=llama3.2
# URL base do provedor local (ex: do Ollama)
OLLAMA_BASE_URL=http://localhost:11434/v1
# Chave de API, caso seu provedor local precise (opcional no Ollama padrão, mas pode requerer algo como "ollama")
API_KEY_LOCAL=ollama

# Configurações para Nuvem / Produção (se IS_LOCAL=false)
# Nome do modelo a ser usado (ex: gpt-4o, claude-3-opus, gemini-1.5-pro, etc.)
MODEL=gemini-2.5-flash

# Chaves de API para os diferentes provedores de IA (preencha as que for utilizar)
OPENAI_API_KEY=sua_chave_openai_aqui
ANTHROPIC_API_KEY=sua_chave_anthropic_aqui
GEMINI_API_KEY=AIzaSyDoztzt__k9NgNsT6HPjSesATDoqwCn8fM

# Logfire
LOGFIRE_API_KEY=pylf_v1_us_Vpqk9ZvJFHJ1TMsQ2fMpTW6kPBdlxGqc8pTcHRGt5qwy
